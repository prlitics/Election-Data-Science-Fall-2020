---
title: "Intro to Prediction"
subtitle: "Election Data Science"  
author: 
  - "Peter Licari"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#1381B0",
  secondary_color = "#FF961C",
  inverse_header_color = "#FFFFFF",
  text_font_google = google_font("Poppins")
)

xaringanExtra::use_xaringan_extra()
library(tidyverse)
library(kableExtra)
```

```{css, echo=FALSE}
pre code, pre, code {
  white-space: pre !important;
  height: 100px !important;}
```

--

```{r, echo=F, fig.align='center', out.width="50%", fig.cap="My prediction"}
knitr::include_graphics("bells.jpg")

```

---
# What we're learning today:
--

1. Cluster analysis
2. Principal component factor analysis
--

3. The intuition for unsupervised exploratory methods, more generally.

---
#Let's tie this into machine learning.

.pull-left[
- There are two\* kinds of machine learning algorithims:
  - **Supervised**
  - **Unsupervised**

- We spent the last two weeks talking about algorithms and tools that fall under the *supervised* umbrella.
- For exploration, we're going to talk about a couple prominent things that fall under the *unsupervised* umbrella.
]

.pull-right[
```{r, echo=F, fig.align='center', out.width="50%"}
knitr::include_graphics("https://miro.medium.com/max/625/1*x7P7gqjo8k2_bj2rTQWAfg.jpeg")
```
]

---
#What's the difference?

.pull-left[
<center><strong>Supervised</strong></center>

- Some kind of ex ante classification or labeling of the data.
  - A person has labeled this observation as a "voter", has inputted how much money they've donated, has determined this picture is a cat, etc.
- Usually a closed-form solution (or relatively consistent algorithm.)
]
.pull-right[<center><strong>Unsupervised</strong><center>

- The data <em>itself</em> provides the information\* that creates the categories. 
- Solutions are usually algorithmic, although some relatively consistent solutions exist.]

--
<center><em>The main way these algorithms ascertain groups is through some conception of <strong>distance</strong></em></center>

---
# Cluster analysis. 
.pull-left[
- Set of observations exist, defined by multiple dimensions.
  - In this case 2.
- The algorithm does 2 things:
  1. Defines the center of clusters. 
  2. Classifies data based upon proximity to the center.
- *You have to define the number of clusters.*
  - There are techniques if you don't know the cluster center, though.
]


.pull-right[
```{r, echo=F, fig.align='center', out.width="50%"}
knitr::include_graphics("https://bookdown.org/rdpeng/exdata/images/kmeans-unnamed-chunk-2-1.png")
```
]

---
# But let's expand our minds about what we mean by "distance"




```{r, echo=FALSE, fig.align='center', out.width="60%"}
knitr::include_graphics("imageassets/stonks.jpg")
```


--

<center> <i>What if I told you I knew a way to forecast tomorrow's weather with about 75-80% accuracy</i> </center>



---

.bg-washed-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
In theory, prediction is relatively easy since the future is not drastically disimilar from the past. But, in theory, prediction is relatively complicated because the future is different in ways we've never experienced in the past.

] 


--

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
In practice, prediction is made even *more* difficult by dint of the fact that the things that are most interesting aren't simple processes, but *complex* or *chaotic* ones. So we're often wrong. (But we still try anyways.)
] 

---

# Two Main "Classes" of Prediction
.pull-left[<center><strong>Regression</strong></center>
- Continuous variables.
- Primarily uses regression to make specific estimates.
  - <i>Tries to get as close to the original value as possible to predict new data.</i>
- <strong><em>Example:</strong></em> Amount of money donated to a candidate via a website.]


.pull-right[<center><strong>Classification</strong></center>
- Discrete and/or categorical variables.
- Uses a host of different techniques: Trees, Regression, SVM, Cluster Analysis...
  - <i>Tries to predict the class of the original data to predict new data.</i>
- <strong><em>Example:</strong></em> Whether someone voted or not; whether someone's ballot will/won't be rejected.]
 
--
<center> In both cases, you are going to specify models with an output variable $Y$ and a series of predictor variables, $X$, with the hope of being better than their most naive predictors (generally expected value). 

---

# Intuition for Predictive Inference:
```{r, fig.align='center', echo=FALSE, out.width= "50%"}
knitr::include_graphics("imageassets/intuition.png")
```

--

<center><i>Implies a relatively continuous stream of new data. This isn't necessarily common in practice</i></center>

---

# Data Alchemy
--

*How do you get "new" data?* **_You use the data you already have._**
--

- You split the data randomly into two groups: *Training data* and **Testing data**.
  - You will do your model building on your training data and your model evaluations through your testing data.
  - Common splits (depending on data size): 90/10, 80/20, 70/30.
- You'll do this **_before_** feature engineering. 

--
- **Doing this carries a number of implicit assumptions about your data-generating process.**

---

# Bias-Variance Trade-off (or Under vs Overfitting)

--

```{r, fig.align='center', out.width="60%", echo=FALSE, fig.cap="Adapted from Boehmke & Greenwell (2020)"}
knitr::include_graphics("imageassets/biasvariance.png")
```
--
<center>
.bg-washed-red.b--dark-red.ba.bw2.br3.shadow-5.ph4.mt5[
 It is <strong>impossible</strong> to avoid this dilemma.] 
</center>

---
# How do we improve underfitting?
--

- Increase the number of predictors in the model (but be smart about it!)
- Don't have your individual-level predictions be dependent on overly large amounts of input from other variables.
- Consider whether you need a parametric versus and non-parametric model.



---
# How do we improve overfitting?
--

.pull-left[
- Curate predicted variables.
- Limit idiosyncratic model behavior.
- Cross-Validation. $\rightarrow$
]

.pull-right[
```{r, echo=F, fig.cap= "Boehmke & Greenwell (2020)"}
knitr::include_graphics("imageassets/xfold.png")
```
]

---
# Prediction Models: Regression cases
--

.pull-left[
- You'll (often) use regression. (Might be a smidge obvious...)
- Hoping to maximize $R^2$ while minimizing mean-squared-error (*MSE*).
  - $R^2$: The amount of the variation in $Y$ "explained" by your $X$ variables.
  - *MSE*: The mean squared error your guesses are from the actual value ("residuals").
- In this case **you're not really interested in the ** $\beta$. 
- Other models: Decision trees, Support Vector Machines, Ridge/LASSO/Elastic Net regression.
]
.pull-right[
```{r, echo=F, fig.cap= "Boehmke & Greenwell (2020)"}
knitr::include_graphics("imageassets/linear.png")
```
]

---
# Classification Models: K - Nearest Neighbor. 
.pull-left[
- Variety of different classification algorithims with their own underlying logic.
- In KNN, you'll use information from other, similar data to estimate unknown values from new data.
- Hoping to minimize *MSE* and minimize *misclassification*
]

.pull-right[
```{r, echo=F, fig.cap= "Italo Jose (2018)"}
knitr::include_graphics("imageassets/knn.png")
```
]