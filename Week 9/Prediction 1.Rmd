---
title: "Intro to Prediction"
subtitle: "Election Data Science"  
author: 
  - "Peter Licari"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#1381B0",
  secondary_color = "#FF961C",
  inverse_header_color = "#FFFFFF",
  text_font_google = google_font("Poppins")
)

xaringanExtra::use_xaringan_extra()
library(tidyverse)
library(kableExtra)
```

```{css, echo=FALSE}
pre code, pre, code {
  white-space: pre !important;
  height: 100px !important;}
```

--

*Passphrase is: *

---

We all like to joke about how poorly weather people perform in their predictions.
--

```{r, echo=FALSE, fig.align='center', out.width="60%"}
knitr::include_graphics("imageassets/stonks.jpg")
```


--

<center> <i>What if I told you I knew a way to forecast tomorrow's weather with about 75-80% accuracy</i> </center>



---

.bg-washed-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[
In theory, prediction is relatively easy since the future is not drastically disimilar from the past. But, in theory, prediction is relatively complicated because the future is different in ways we've never experienced in the past.

] 


--

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
In practice, prediction is made even *more* difficult by dint of the fact that the things that are most interesting aren't simple processes, but *complex* or *chaotic* ones. So we're often wrong. (But we still try anyways.)
] 

---

# Two Main "Classes" of Prediction
.pull-left[<center><strong>Regression</strong></center>
- Continuous variables.
- Primarily uses regression to make specific estimates.
  - <i>Tries to get as close to the original value as possible to predict new data.</i>
- <strong><em>Example:</strong></em> Amount of money donated to a candidate via a website.]


.pull-right[<center><strong>Classification</strong></center>
- Discrete and/or categorical variables.
- Uses a host of different techniques: Trees, Regression, SVM, Cluster Analysis...
  - <i>Tries to predict the class of the original data to predict new data.</i>
- <strong><em>Example:</strong></em> Whether someone voted or not; whether someone's ballot will/won't be rejected.]
 
--
<center> In both cases, you are going to specify models with an output variable $Y$ and a series of predictor variables, $X$, with the hope of being better than their most naive predictors (generally expected value). 

---

# Intuition for Predictive Inference:
```{r, fig.align='center', echo=FALSE, out.width= "50%"}
knitr::include_graphics("imageassets/intuition.png")
```

--

<center><i>Implies a relatively continuous stream of new data. This isn't necessarily common in practice</i></center>

---

# Data Alchemy
--

*How do you get "new" data?* **_You use the data you already have._**
--

- You split the data randomly into two groups: *Training data* and **Testing data**.
  - You will do your model building on your training data and your model evaluations through your testing data.
  - Common splits (depending on data size): 90/10, 80/20, 70/30.
- You'll do this **_before_** feature engineering. 

--
- **Doing this carries a number of implicit assumptions about your data-generating process.**

---

# Bias-Variance Trade-off (or Under vs Overfitting)

--

```{r, fig.align='center', out.width="60%", echo=FALSE, fig.cap="Adapted from Boehmke & Greenwell (2020)"}
knitr::include_graphics("imageassets/biasvariance.png")
```
--
<center>
.bg-washed-red.b--dark-red.ba.bw2.br3.shadow-5.ph4.mt5[
 It is <strong>impossible</strong> to avoid this dilemma.] 
</center>

---
# How do we improve underfitting?
--

- Increase the number of predictors in the model (but be smart about it!)
- Don't have your individual-level predictions be dependent on overly large amounts of input from other variables.
- Consider whether you need a parametric versus and non-parametric model.



---
# How do we improve overfitting?
--

.pull-left[
- Curate predicted variables.
- Limit idiosyncratic model behavior.
- Cross-Validation. $\rightarrow$
]

.pull-right[
```{r, echo=F, fig.cap= "Boehmke & Greenwell (2020)"}
knitr::include_graphics("imageassets/xfold.png")
```
]

---
# Prediction Models: Regression cases
--

.pull-left[
- You'll (often) use regression. (Might be a smidge obvious...)
- Hoping to maximize $R^2$ while minimizing mean-squared-error (*MSE*).
  - $R^2$: The amount of the variation in $Y$ "explained" by your $X$ variables.
  - *MSE*: The mean squared error your guesses are from the actual value ("residuals").
- In this case **you're not really interested in the ** $\beta$. 
- Other models: Decision trees, Support Vector Machines, Ridge/LASSO/Elastic Net regression.
]
.pull-right[
```{r, echo=F, fig.cap= "Boehmke & Greenwell (2020)"}
knitr::include_graphics("imageassets/linear.png")
```
]

---
# Classification Models: K - Nearest Neighbor. 
.pull-left[
- Variety of different classification algorithims with their own underlying logic.
- In KNN, you'll use information from other, similar data to estimate unknown values from new data.
- Hoping to minimize *MSE* and minimize *misclassification*
]

.pull-right[
```{r, echo=F, fig.cap= "Italo Jose (2018)"}
knitr::include_graphics("imageassets/knn.png")
```
]